{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# План:\n",
    "1. Python\n",
    "2. Numpy\n",
    "3. Pandas\n",
    "4. Regexp + regexp with dataframes\n",
    "5. <b>Texts</b>\n",
    "6. Matplotlib, Seaborn\n",
    "7. Classification, clustering, binary, multiclass, multilabel\n",
    "8. Metrics\n",
    "9. Feature extraction\n",
    "10. Pyspark/sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEXTS\n",
    "\n",
    "Рассмотрим:\n",
    "\n",
    "* предобработку текста\n",
    "* представление текста\n",
    "* понятие эмбеддинга\n",
    "* текстовую классификацию"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://media.giphy.com/media/nopqz91prOyvS/giphy.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Предобработка текста\n",
    "\n",
    "Текст на естественном языке, который нужно обрабатывать в задачах машинного обучения, сильно зависит от источника. Пример:\n",
    "\n",
    "Википедия\n",
    "> Литературный язык — обработанная часть общенародного языка, обладающая в большей или меньшей степени письменно закреплёнными нормами; язык всех проявлений культуры, выражающихся в словесной форме.\n",
    "\n",
    "Твиттер\n",
    "> Если у вас в компании есть люди, которые целый день сидят в чатиках и смотрят видосики, то, скорее всего, это ДАТАСАЕНТИСТЫ и у них ОБУЧАЕТСЯ\n",
    "\n",
    "Ответы@Mail.ru\n",
    "> как пишется \"Вообщем лето было отличное\" раздельно или слитно слово ВОобщем?? ?\n",
    "\n",
    "В связи с этим, возникает задача предобработки (или нормализации) текста, то есть приведения к некоторому единому виду."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Приведение текста к нижнему регистру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'купил таблетки от тупости, но не смог открыть банку,ЧТО ДЕЛАТЬ???'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'купил таблетки от тупости, но не смог открыть банку,что делать???'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Удаление неинформативных символов.\n",
    "\n",
    "Такими символами могут быть символы пунктуации, спец-символы, повторяющиеся символы, цифры.\n",
    "Удалите символы пунктуации и лишние пробелы из предыдущего текста в нижнем регистре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'(\\?|,)'\n",
    "text = ' '.join(re.sub(pattern, ' ', text).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Разбиение текста на смысловые единицы (токенизация)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 300 рублей. До 01.01.2020.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к токенизации - это разбиение по текста по пробельным символам. \n",
    "\n",
    "**Quiz: Какая у этого подхода есть проблема?**\n",
    "\n",
    "- Объем словаря будет большим. В текстах могут встретится как слова **кружка**, **термос** по отдельности так и через тире **кружка-термос** - и при использовании простых способов векторизации текстов это будут 3 разных смысловых единицы, хотя логично было бы оставить 2.\n",
    "\n",
    "- Если перед нами стоит задача, в которой необходимо учитывать пунктуацию, то нам придется придумывать способ оторвать точки, кавычки, скобки от слов\n",
    "    \n",
    "- Если пробелы пропущены, то два слова могут остаться склееными через запятую\\точку\\дефис"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Другие способы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В библиотеке для морфологического анализа для русского языка [`pymorphy2`](https://pymorphy2.readthedocs.io/en/latest/) есть простая вспомогательная функция для токенизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2.tokenizers import simple_word_tokenize\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0',\n",
       " '.',\n",
       " '5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '300',\n",
       " 'рублей',\n",
       " '.',\n",
       " 'До',\n",
       " '01',\n",
       " '.',\n",
       " '01',\n",
       " '.',\n",
       " '2020',\n",
       " '.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_word_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более сложной метод токенизации представлен в [`nltk`](https://www.nltk.org/): библиотеке для общего NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize, wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Сравните и напишите в комментарии чем отличаются эти три метода**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите кружку-термос \"Hello Kitty\" на 0.5л (64см³) за 300 рублей.',\n",
       " 'До 01.01.2020.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '``',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " \"''\",\n",
       " 'на',\n",
       " '0.5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '300',\n",
       " 'рублей',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2020',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку',\n",
       " '-',\n",
       " 'термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0',\n",
       " '.',\n",
       " '5л',\n",
       " '(',\n",
       " '64см³',\n",
       " ')',\n",
       " 'за',\n",
       " '300',\n",
       " 'рублей',\n",
       " '.',\n",
       " 'До',\n",
       " '01',\n",
       " '.',\n",
       " '01',\n",
       " '.',\n",
       " '2020',\n",
       " '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_tokenize\n",
    "# возращает токены по предложениям, разделенные по точке\n",
    "\n",
    "# word_tokenize\n",
    "# возвращает токены в виде слов разделленных по пробелу\n",
    "\n",
    "# wordpunct_tokenize\n",
    "# возвращает токены в виде слов разделенных по пробелу точке пунктиру (специальному символу)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка также есть новая специализированная библиотека [`razdel`](https://github.com/natasha/razdel).\n",
    "\n",
    "**Напишите функцию, которая принимает на вход текст и возвращает список токенов из метода tokenize библиотеки razdel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Купите',\n",
       " 'кружку-термос',\n",
       " '\"',\n",
       " 'Hello',\n",
       " 'Kitty',\n",
       " '\"',\n",
       " 'на',\n",
       " '0.5',\n",
       " 'л',\n",
       " '(',\n",
       " '64',\n",
       " 'см³',\n",
       " ')',\n",
       " 'за',\n",
       " '300',\n",
       " 'рублей',\n",
       " '.',\n",
       " 'До',\n",
       " '01.01.2020',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from razdel import tokenize\n",
    "\n",
    "\n",
    "def tokenize_with_razdel(text):\n",
    "    tokens = list(tokenize(text))\n",
    "    tokens = [_.text for _ in tokens]\n",
    "    return tokens\n",
    "tokenize_with_razdel(text)\n",
    "\n",
    "## Но я не понял как это работает!\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Приведение слов к нормальной форме (стемминг, лемматизация)\n",
    "\n",
    "**Стемминг - это нормализация слова путём отбрасывания окончания по правилам языка.**\n",
    "\n",
    "Такая нормализация хорошо подходит для языков с небольшим разнообразием словоформ, например, для английского. В библиотеке nltk есть несколько реализаций стеммеров:\n",
    " - Porter stemmer\n",
    " - Snowball stemmer - только его можно использовать для русского языка (но лучше не надо)\n",
    " - Lancaster stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "SnowballStemmer(language='english').stem('running')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для русского языка этот подход не очень подходит, поскольку в русском есть падежные формы, время у глаголов и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'бежа'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SnowballStemmer(language='russian').stem('бежать')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Лемматизация - приведение слов к начальной морфологической форме (с помощью словаря и грамматики языка).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Две самые часто используемые библиотеки для лемматизации русских слов:\n",
    "- [pymorphy2](https://pymorphy2.readthedocs.io/en/latest/)\n",
    "- [mystem3](https://tech.yandex.ru/mystem/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самый простой подход к лемматизации - словарный. Здесь не учитывается контекст слова, поэтому для омонимов такой подход работает не всегда. Такой подход применяет библиотека pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "pymorphy = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход список токенов и возвращаюшую список лемматизированных с помощью pymorphy токенов**\n",
    "\n",
    "протестируйте на примере 'на заводе стали увидел виды стали'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_with_pymorphy(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = pymorphy.parse(tokens[i])[0].normal_form\n",
    "    return(tokens) ##your code\n",
    "\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['на', 'завод', 'стать', 'увидеть', 'вид', 'стать']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'на заводе стали увидел виды стали'\n",
    "lemmatize_with_pymorphy(tokenize_with_razdel(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Библиотека от Яндекса `mystem3` обходит это ограничение и рассматривает контекст слова, используя статистику и правила. + Имеет свой токенизатор"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Напишите функцию принимающую на вход строку и возвращаюшую список лемматизированных токенов с помощью pymystem**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'на заводе стали увидел виды стали'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1260] Эта программа заблокирована групповой политикой. За дополнительными сведениями обращайтесь к системному администратору",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4961ef5a0490>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m##your code\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mmystem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36mlemmatize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[0mneed_encode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0minfos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m         \u001b[0mlemmas\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36manalyze\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    249\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 250\u001b[1;33m             \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_analyze_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    251\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_analyze_impl\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_mystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\site-packages\\pymystem3\\mystem.py\u001b[0m in \u001b[0;36m_start_mystem\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    230\u001b[0m                                       \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m                                       \u001b[0mbufsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m                                       close_fds=True if _POSIX else False)\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_procout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors)\u001b[0m\n\u001b[0;32m    705\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    706\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 707\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    708\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m    988\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    989\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 990\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m    991\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [WinError 1260] Эта программа заблокирована групповой политикой. За дополнительными сведениями обращайтесь к системному администратору"
     ]
    }
   ],
   "source": [
    "from pymystem3 import Mystem\n",
    "\n",
    "mystem = Mystem()\n",
    "\n",
    "\n",
    "# def lemmatize_with_mystem(text):\n",
    "    ##your code\n",
    "    \n",
    "##your code\n",
    "mystem.lemmatize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще более крутая и более **медленная** библиотека `RNNMorph` базирующаяся на рекурентных сетях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'на заводе стали увидел виды стали'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rnnmorph.predictor import RNNMorphPredictor\n",
    "predictor = RNNMorphPredictor(language=\"ru\")\n",
    "\n",
    "# def lemmatize_with_rnnmorph(tokens):\n",
    "    #you know what to do\n",
    "pre\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Представление текста"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 One-Hot Encoding\n",
    "<img src=\"gifs/one_hot.png\" width=\"500\">\n",
    "\n",
    "При таком способе представления текстов, составляется словарь всех слов со всех документов - столбцы нашей матрицы, строки это документы. \n",
    "\n",
    "Если слово есть в документе на пересечении столбца и строки ставится 1, иначе 0. \n",
    "\n",
    "Матрица сформированная таким образом получается разреженной, т.е. если у нас очень много уникальных слов, доля единиц в строке будет маленькой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ранее мы уже получали one-hot вектора с помощью pandas\\numpy. \n",
    "Сначала нам нужно каждому слову поставить в соответствие номер, а затем перевести их в бинарные вектора. \n",
    "\n",
    "В этот раз используем библиотеку scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "words = ['What', 'the', 'hell']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Получите one-hot вектора используя LabelEncoder и OneHotEncoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(words)\n",
    "le.transform(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mas = np.array(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t1.0\n",
      "  (2, 1)\t1.0\n"
     ]
    }
   ],
   "source": [
    "ohe = OneHotEncoder()\n",
    "value = ohe.fit_transform(np.array(words).reshape(-1,1))\n",
    "print(value)\n",
    "# ohe.transform(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Что будет, если мы сложим все one-hot вектора слов в тексте?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['What']], dtype='<U4')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe.inverse_transform([[3,0,0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Bag-of-words\n",
    "\n",
    "В прошлом методе, если слово употребляется в тексте на пересечении столбца-слова и строки-документа ставились 1, но теперь мы бы хотели так же знать сколько раз слово встретилось в данном документе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы посчитать количество слов в тексте, используем метод CountVectorizer из sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'Кот пьет молоко',\n",
    "    'Кто пьет молоко?',\n",
    "    'Молоко выпивается котом',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Обучите CountVectorizer на примерах, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['выпивается', 'кот', 'котом', 'кто', 'молоко', 'пьет']\n",
      "[[0 1 0 0 1 1]\n",
      " [0 0 0 1 1 1]\n",
      " [1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "print(cv.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 TF-IDF\n",
    "\n",
    "**Term Frequency**  $tf(w,d)$ - сколько раз слово $w$ встретилось в документе $d$\n",
    "\n",
    "**Document Frequency** $df(w)$ - сколько документов содержат слово $w$\n",
    "\n",
    "**Inverse Document Frequency** $idf(w) = log_2(N/df(w))$  — обратная документная частотность. \n",
    "\n",
    "**TF-IDF**=$tf(w,d)*idf(w)$\n",
    "\n",
    "В гите есть презентация, с которой возможно, станет понятнее **`векторайзеры и метрики.pptx`**\n",
    "\n",
    "**Обучите TfidfVectorizer на примерах из предыдущего задания, выведите вектора для трех предложений и список слов-столбцов матрицы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Кот пьет молоко', 'Кто пьет молоко?', 'Молоко выпивается котом']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['выпивается', 'кот', 'котом', 'кто', 'молоко', 'пьет']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.72033345, 0.        , 0.        , 0.42544054,\n",
       "        0.54783215],\n",
       "       [0.        , 0.        , 0.        , 0.72033345, 0.42544054,\n",
       "        0.54783215],\n",
       "       [0.65249088, 0.        , 0.65249088, 0.        , 0.38537163,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классификация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы попробуем применить описание методы предобработки и представления текста на примере анализа тональности текста. В качестве данных будем использовать небольшой датасет твитов. Всего в данных 2 класса: позитив и негатив."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Загрузка данных и получение тренировочной и тестовой выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6929, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('data/train.csv')\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>эти розы для прекрасной мамочки)))=_=]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>И да, у меня в этом году серьезные проблемы со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>♥Обожаю людей, которые заставляют меня смеятьс...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>Вчера нашла в почтовом ящике пустую упаковку и...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>очень долгожданный и хороший день был)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                               text\n",
       "0  positive            эти розы для прекрасной мамочки)))=_=]]\n",
       "1  negative  И да, у меня в этом году серьезные проблемы со...\n",
       "2  positive  ♥Обожаю людей, которые заставляют меня смеятьс...\n",
       "3  negative  Вчера нашла в почтовом ящике пустую упаковку и...\n",
       "4  positive             очень долгожданный и хороший день был)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Разбейте выборку на две тренировочную и валидационную с помощью функции train_test_split из sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train['label'], train['text'], stratify=train['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Проверьте что доли классов на train и на test совпадают**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5196,)\n",
      "(1733,)\n",
      "(5196,)\n",
      "(1733,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Оценка качества"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша выборка не сбалансирована (доля одно из класса значительно ниже доли другого), поэтому стандартные метрики качества для классификаторов вроде accuracy или roc auc нам не подходят"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам нужна Точность (Precision) и Полнота (Recall)!\n",
    "\n",
    "**Про эти метрики можно подробно почитать в презентации  `векторайзеры и метрики.pptx`**\n",
    "\n",
    "Для подсчета метрик будем использовать говоторые функции из sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для оценки векторизатора. В качестве модели будем использовать линейный SVM, он хорошо работает на задачах классификации текстов, когда для векторизации используются методы дающие разреженные матрицы.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Реализуйте функцию следуя инструкциям в комментариях**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import tqdm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "def evaluate_vectorizer(vectorizer):\n",
    "    #обучение выбранного векторайзера и получение веторов текстов (fit_transform)  \n",
    "    clf = LinearSVC()\n",
    "    clf.fit_transform(vectorizer)\n",
    "    #иницилизация классификатора и тренировка модели\n",
    "    \n",
    "    \n",
    "    #получение векторов текстов для теста\n",
    "    \n",
    "    #получение предсказания\n",
    "    \n",
    "    #вывод метрик классификации с помощью функции classification_report - выводит precision и recall для каждого класса\n",
    "    \n",
    "    #возвращаем предсказанные классы для теста\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Сравнение способов представления текста\n",
    "\n",
    "* необработанный текст + используйте CountVectorizer\n",
    "* переведите в lower и используйте CountVectorizer\n",
    "* lower + lemmatization + CountVectorizer\n",
    "* lower + stemming (SnowballStemer) + CountVectorizer\n",
    "Выберите лучший из двух последних ->\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + CountVectorizer + передавайте в векторайзер свой токенизатор (например из razdel) + передайте список стоп слов (data/stopwords-ru.txt)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer + передавайте в векторайзер свой токенизатор (например из razdel)\n",
    "* lower + lemmatization\\stemming + TfidfVectorizer (используйте ngrams(2, 2)) + передавайте в векторайзер свой токенизатор (например из razdel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from pymorphy2.tokenizers import simple_word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"югославской историографии также известна как «Седьмое вражеское наступление» или Десант на Дрвар (сербохорв. Десант на Дрвар / Desant na Drvar) — комбинированная воздушно-десантная и сухопутная наступательная операция войск 2-й танковой армии вермахта во время Второй мировой войны. Проводилась в Западной Боснии в районе Бугойно — Яйце — Баня-Лука — Приедор — Бихач — Книн в период с 25 мая по 6 июня 1944 года с целью уничтожения Верховного штаба НОАЮ в городе Дрвар, а также находившихся при нём учреждений народно-освободительного движения Югославии и союзных военных миссий. В операции участвовали 500-й парашютно-десантный батальон СС, а также части 15-го горнопехотного армейского корпуса и 5-го горнопехотного корпуса СС.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### необработанный текст + используйте CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15',\n",
       " '1944',\n",
       " '25',\n",
       " '500',\n",
       " 'desant',\n",
       " 'drvar',\n",
       " 'na',\n",
       " 'армейского',\n",
       " 'армии',\n",
       " 'баня',\n",
       " 'батальон',\n",
       " 'бихач',\n",
       " 'боснии',\n",
       " 'бугойно',\n",
       " 'вермахта',\n",
       " 'верховного',\n",
       " 'во',\n",
       " 'военных',\n",
       " 'воздушно',\n",
       " 'войны',\n",
       " 'войск',\n",
       " 'вражеское',\n",
       " 'время',\n",
       " 'второй',\n",
       " 'го',\n",
       " 'года',\n",
       " 'горнопехотного',\n",
       " 'городе',\n",
       " 'движения',\n",
       " 'десант',\n",
       " 'десантная',\n",
       " 'десантный',\n",
       " 'дрвар',\n",
       " 'западной',\n",
       " 'известна',\n",
       " 'или',\n",
       " 'историографии',\n",
       " 'июня',\n",
       " 'как',\n",
       " 'книн',\n",
       " 'комбинированная',\n",
       " 'корпуса',\n",
       " 'лука',\n",
       " 'мая',\n",
       " 'мировой',\n",
       " 'миссий',\n",
       " 'на',\n",
       " 'народно',\n",
       " 'наступательная',\n",
       " 'наступление',\n",
       " 'находившихся',\n",
       " 'ноаю',\n",
       " 'нём',\n",
       " 'операции',\n",
       " 'операция',\n",
       " 'освободительного',\n",
       " 'парашютно',\n",
       " 'период',\n",
       " 'по',\n",
       " 'при',\n",
       " 'приедор',\n",
       " 'проводилась',\n",
       " 'районе',\n",
       " 'седьмое',\n",
       " 'сербохорв',\n",
       " 'союзных',\n",
       " 'сс',\n",
       " 'сухопутная',\n",
       " 'также',\n",
       " 'танковой',\n",
       " 'уничтожения',\n",
       " 'участвовали',\n",
       " 'учреждений',\n",
       " 'целью',\n",
       " 'части',\n",
       " 'штаба',\n",
       " 'югославии',\n",
       " 'югославской',\n",
       " 'яйце']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform([text])\n",
    "cv.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# переведите в lower и используйте CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15',\n",
       " '1944',\n",
       " '25',\n",
       " '500',\n",
       " 'desant',\n",
       " 'drvar',\n",
       " 'na',\n",
       " 'армейского',\n",
       " 'армии',\n",
       " 'баня',\n",
       " 'батальон',\n",
       " 'бихач',\n",
       " 'боснии',\n",
       " 'бугойно',\n",
       " 'вермахта',\n",
       " 'верховного',\n",
       " 'во',\n",
       " 'военных',\n",
       " 'воздушно',\n",
       " 'войны',\n",
       " 'войск',\n",
       " 'вражеское',\n",
       " 'время',\n",
       " 'второй',\n",
       " 'го',\n",
       " 'года',\n",
       " 'горнопехотного',\n",
       " 'городе',\n",
       " 'движения',\n",
       " 'десант',\n",
       " 'десантная',\n",
       " 'десантный',\n",
       " 'дрвар',\n",
       " 'западной',\n",
       " 'известна',\n",
       " 'или',\n",
       " 'историографии',\n",
       " 'июня',\n",
       " 'как',\n",
       " 'книн',\n",
       " 'комбинированная',\n",
       " 'корпуса',\n",
       " 'лука',\n",
       " 'мая',\n",
       " 'мировой',\n",
       " 'миссий',\n",
       " 'на',\n",
       " 'народно',\n",
       " 'наступательная',\n",
       " 'наступление',\n",
       " 'находившихся',\n",
       " 'ноаю',\n",
       " 'нём',\n",
       " 'операции',\n",
       " 'операция',\n",
       " 'освободительного',\n",
       " 'парашютно',\n",
       " 'период',\n",
       " 'по',\n",
       " 'при',\n",
       " 'приедор',\n",
       " 'проводилась',\n",
       " 'районе',\n",
       " 'седьмое',\n",
       " 'сербохорв',\n",
       " 'союзных',\n",
       " 'сс',\n",
       " 'сухопутная',\n",
       " 'также',\n",
       " 'танковой',\n",
       " 'уничтожения',\n",
       " 'участвовали',\n",
       " 'учреждений',\n",
       " 'целью',\n",
       " 'части',\n",
       " 'штаба',\n",
       " 'югославии',\n",
       " 'югославской',\n",
       " 'яйце']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform([text.lower()])\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 2, 1, 2, 1, 1, 2, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1,\n",
       "        1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lower + lemmatization + CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = simple_word_tokenize(text.lower())\n",
    "pymorph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(corpus)):\n",
    "    corpus[i] = pymorph.parse(corpus[i])[0].normal_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text = ' '.join(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform([new_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['15',\n",
       " '1944',\n",
       " '25',\n",
       " '500',\n",
       " 'desant',\n",
       " 'drvar',\n",
       " 'na',\n",
       " 'армейский',\n",
       " 'армия',\n",
       " 'баня',\n",
       " 'батальон',\n",
       " 'бихач',\n",
       " 'босния',\n",
       " 'бугойный',\n",
       " 'вермахт',\n",
       " 'верховный',\n",
       " 'военный',\n",
       " 'воздушно',\n",
       " 'война',\n",
       " 'войско',\n",
       " 'вражеский',\n",
       " 'время',\n",
       " 'го',\n",
       " 'год',\n",
       " 'горнопехотный',\n",
       " 'город',\n",
       " 'два',\n",
       " 'движение',\n",
       " 'десант',\n",
       " 'десантный',\n",
       " 'дрвар',\n",
       " 'западный',\n",
       " 'известный',\n",
       " 'или',\n",
       " 'историография',\n",
       " 'июнь',\n",
       " 'как',\n",
       " 'книна',\n",
       " 'комбинированный',\n",
       " 'корпус',\n",
       " 'лука',\n",
       " 'май',\n",
       " 'мировой',\n",
       " 'миссия',\n",
       " 'на',\n",
       " 'народно',\n",
       " 'наступательный',\n",
       " 'наступление',\n",
       " 'находиться',\n",
       " 'ноать',\n",
       " 'он',\n",
       " 'операция',\n",
       " 'освободительный',\n",
       " 'парашютно',\n",
       " 'период',\n",
       " 'по',\n",
       " 'при',\n",
       " 'приедора',\n",
       " 'проводиться',\n",
       " 'район',\n",
       " 'семь',\n",
       " 'сербохорв',\n",
       " 'союзный',\n",
       " 'сс',\n",
       " 'сухопутный',\n",
       " 'также',\n",
       " 'танковый',\n",
       " 'ть',\n",
       " 'уничтожение',\n",
       " 'участвовать',\n",
       " 'учреждение',\n",
       " 'цель',\n",
       " 'часть',\n",
       " 'штаб',\n",
       " 'югославия',\n",
       " 'югославский',\n",
       " 'яйцо']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        2, 1, 2, 1, 1, 1, 2, 2, 3, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,\n",
       "        2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 3,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lower + stemming (SnowballStemer) + CountVectorizer Выберите лучший из двух последних ->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
